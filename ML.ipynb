{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while ($true) {\n",
    "#     nvidia-smi\n",
    "#     Start-Sleep -Seconds 1\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уже подготовленный датасет\n",
    "dataset = r'D:\\Veeple\\IT\\Python_scripts\\Scripts\\Обучение aidar_musin\\инфа из Согласия\\try_2\\mixed_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "    mixed_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущие метки модели: {0: 'O', 1: 'B-ZipCode', 2: 'I-ZipCode', 3: 'B-Country', 4: 'I-Country', 5: 'B-Region', 6: 'I-Region', 7: 'B-District', 8: 'I-District', 9: 'B-Settlement', 10: 'I-Settlement', 11: 'B-Street', 12: 'I-Street', 13: 'B-House', 14: 'I-House', 15: 'B-Building', 16: 'I-Building', 17: 'B-Apartment', 18: 'I-Apartment'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model_name = \"aidarmusin/address-ner-ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Текущие метки модели\n",
    "current_labels = model.config.id2label\n",
    "print(\"Текущие метки модели:\", current_labels)\n",
    "\n",
    "# Добавление новых меток\n",
    "new_labels = {\n",
    "    0: \"O\",\n",
    "    1: \"B-ZipCode\",\n",
    "    2: \"I-ZipCode\",\n",
    "    3: \"B-Country\",\n",
    "    4: \"I-Country\",\n",
    "    5: \"B-Region\",\n",
    "    6: \"I-Region\",\n",
    "    7: \"B-District\",\n",
    "    8: \"I-District\",\n",
    "    9: \"B-Settlement\",\n",
    "    10: \"I-Settlement\",\n",
    "    11: \"B-Street\",\n",
    "    12: \"I-Street\",\n",
    "    13: \"B-House\",\n",
    "    14: \"I-House\",\n",
    "    15: \"B-Building\",\n",
    "    16: \"I-Building\",\n",
    "    17: \"B-Apartment\",\n",
    "    18: \"I-Apartment\",\n",
    "    19: \"B-Organization\",  # Новая метка\n",
    "    20: \"I-Organization\",  # Новая метка\n",
    "}\n",
    "\n",
    "# Обновление конфигурации\n",
    "model.config.id2label = new_labels\n",
    "model.config.label2id = {v: k for k, v in new_labels.items()}\n",
    "\n",
    "# Увеличиваем количество классов в модели\n",
    "model.num_labels = len(new_labels)\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset, tokenizer, model, max_length=64):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        address = item[\"address\"]\n",
    "        entities = item[\"extracted_entities\"]\n",
    "        \n",
    "        # Токенизация с разбиением на токены и их смещениями\n",
    "        encoding = tokenizer(\n",
    "            address,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True  # Важно! Нам нужны индексы токенов\n",
    "        )\n",
    "        \n",
    "        offset_mapping = encoding[\"offset_mapping\"].squeeze(0)  # Получаем смещения токенов\n",
    "        label_ids = [0] * max_length  # Все токены по умолчанию \"O\"\n",
    "        \n",
    "        for entity in entities:\n",
    "            ent_start, ent_end = entity[\"start\"], entity[\"end\"]\n",
    "            ent_type = entity[\"type\"]\n",
    "            \n",
    "            # Получаем ID для сущности (если нет в словаре, используем \"O\")\n",
    "            label_id_B = model.config.label2id.get(f\"B-{ent_type}\", 0)\n",
    "            label_id_I = model.config.label2id.get(f\"I-{ent_type}\", 0)\n",
    "            \n",
    "            # Назначаем метки для соответствующих токенов\n",
    "            for i, (tok_start, tok_end) in enumerate(offset_mapping):\n",
    "                if tok_start == 0 and tok_end == 0:\n",
    "                    continue  # Пропускаем паддинги\n",
    "                if tok_start >= ent_start and tok_end <= ent_end:\n",
    "                    if label_ids[i] == 0:  # Если еще не размечен\n",
    "                        label_ids[i] = label_id_B  # Первый токен как \"B-\"\n",
    "                    else:\n",
    "                        label_ids[i] = label_id_I  # Остальные \"I-\"\n",
    "        \n",
    "        labels.append(torch.tensor(label_ids))\n",
    "        inputs.append(encoding)\n",
    "    \n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "max_length = 64  # Максимальная длина последовательности\n",
    "inputs, labels = prepare_data(mixed_dataset, tokenizer, model, max_length=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"].squeeze(),  # Убираем лишнюю размерность\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"].squeeze(),\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Создание датасета\n",
    "train_dataset = CustomDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделение данных на обучающую и оценочную части\n",
    "train_data, eval_data = train_test_split(mixed_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Подготовка обучающего набора данных\n",
    "train_inputs, train_labels = prepare_data(train_data, tokenizer, model, max_length=64)\n",
    "train_dataset = CustomDataset(train_inputs, train_labels)\n",
    "\n",
    "# Подготовка оценочного набора данных\n",
    "eval_inputs, eval_labels = prepare_data(eval_data, tokenizer, model, max_length=64)\n",
    "eval_dataset = CustomDataset(eval_inputs, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Определение TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # Каталог для сохранения модели\n",
    "    evaluation_strategy=\"epoch\",       # Оценка после каждой эпохи\n",
    "    save_strategy=\"epoch\",             # Сохранение модели после каждой эпохи\n",
    "    learning_rate=5e-5,                # Можно немного уменьшить, т.к. дообучаем\n",
    "    per_device_train_batch_size=16,    # Можно уменьшить до 8, если памяти не хватает\n",
    "    per_device_eval_batch_size=16,     # Аналогично для оценки\n",
    "    num_train_epochs=5,                # 5 эпох — неплохой баланс\n",
    "    weight_decay=0.01,                 # Регуляризация для предотвращения переобучения\n",
    "    save_total_limit=2,                 # Храним только 2 последние версии модели\n",
    "    logging_dir=\"./logs\",              # Каталог для логов\n",
    "    logging_steps=10,                   # Логирование каждые 10 шагов\n",
    "    warmup_steps=500,                   # Количество шагов для разогрева learning rate\n",
    "    fp16=True,                          # Используем 16-битные тензоры (ускоряет обучение на GPU)\n",
    "    gradient_accumulation_steps=2,      # Сглаживание градиентов, если маленький batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем модель и токенизатор в указанную папку\n",
    "model.save_pretrained(\"./trained_try_2\")\n",
    "tokenizer.save_pretrained(\"./trained_try_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Загружаем свою дообученную модель и токенизатор\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./trained_try_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Или свой токенизатор, если использовался другой\n",
    "\n",
    "# Инициализация pipeline для NER с обученной моделью\n",
    "address_ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=-1)  # device=0 для GPU, device=-1 для CPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Адрес для предсказания\n",
    "address = \"ООО Ромашка на Тверской ул. Тверская, д. 1\"\n",
    "\n",
    "# Применяем pipeline для извлечения сущностей\n",
    "entities = address_ner_pipeline(address)\n",
    "\n",
    "# Выводим результат\n",
    "print(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
